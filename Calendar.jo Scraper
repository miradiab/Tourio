import requests
from bs4 import BeautifulSoup
import pandas as pd

BASE_URL = "https://calendar.jo/"

def get_page(url):
    resp = requests.get(url)
    resp.raise_for_status()
    return BeautifulSoup(resp.text, "html.parser")

def get_event_links():
    """Scrape homepage for event links"""
    soup = get_page(BASE_URL)
    links = []
    for a in soup.select("a[href^='/events/']"):
        full_url = BASE_URL.rstrip("/") + a["href"]
        if full_url not in links:
            links.append(full_url)
    return links

def scrape_event(url):
    """Scrape event details from a single page"""
    soup = get_page(url)
    title = soup.find("h1").get_text(strip=True) if soup.find("h1") else "Untitled"
    date = soup.find("p").get_text(strip=True) if soup.find("p") else ""
    location = None
    for p in soup.find_all("p"):
        if "Amman" in p.get_text() or "Aqaba" in p.get_text():
            location = p.get_text(strip=True)
            break
    return {
        "url": url,
        "title": title,
        "date": date,
        "location": location
    }

def main():
    event_links = get_event_links()
    events = []
    for link in event_links:
        try:
            events.append(scrape_event(link))
        except Exception as e:
            print(f"Failed {link}: {e}")
    # Save to CSV
    df = pd.DataFrame(events)
    df.to_csv("calendar_events.csv", index=False, encoding="utf-8")
    print(f"Saved {len(events)} events to calendar_events.csv")

if __name__ == "__main__":
    main()
